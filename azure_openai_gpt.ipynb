{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/amadeus-art/azure-openai-coding-dojo/blob/main/azure_openai_gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "# Azure OpenAI API: Call GPTs from Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install openai python-dotenv tiktoken -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Environment Setup\n",
    "Before executing the following cells, make sure to set the `AZURE_OPENAI_KEY` and `AZURE_OPENAI_ENDPOINT` variables in the `.env` file or export them.\n",
    "\n",
    "<br/>\n",
    "<img src=\"assets/keys_endpoint.png\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from openai import OpenAIError\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv(), override=True) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_key = os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint  = os.getenv(\"AZURE_OPENAI_ENDPOINT\"), # should look like the following # https://YOUR_RESOURCE_NAME.openai.azure.com/\n",
    "    api_version = '2024-02-01', # latest as per today (15-09-2023), may change in the future\n",
    ")\n",
    "\n",
    "# this is the name of the deployments you created in the Azure portal within the above resource\n",
    "deployment_name = os.getenv(\"MODEL_DEPLOYMENT_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tokenization\n",
    "Tokens can be thought of as pieces of words. Before the API processes the prompts, the input is broken down into tokens. These tokens are not cut up exactly where the words start or end - tokens can include trailing spaces and even sub-words. Here are some helpful rules of thumb for understanding tokens in terms of lengths:\n",
    "- 1 token ~= 4 chars in English\n",
    "- 1 token ~= ¾ words\n",
    "- 100 tokens ~= 75 words\n",
    "\n",
    "How words are split into tokens is language-dependent.\n",
    "Depending on the model used, requests can use up to 4097 tokens shared between prompt and completion. If your prompt is 4000 tokens, your completion can be 97 tokens at most.\n",
    "The limit is currently a technical limitation, but there are often creative ways to solve problems within the limit, e.g. condensing your prompt, breaking the text into smaller pieces, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text:\n",
      "[198, 1999, 1463, 279, 2768, 3754, 32263, 1139, 220, 16, 315, 279, 2768, 11306, 25, 8184, 11, 17829, 11, 35979, 11, 18707, 11, 23334, 271, 12626, 1074, 220, 16, 25, 47863, 3441, 544, 35117, 2209, 57410, 3216, 264, 1561, 17262, 315, 3700, 13421, 13, 578, 8191, 596, 1455, 28530, 17677, 60526, 706, 264, 93254, 4341, 502, 2363, 323, 264, 7878, 502, 13356, 198, 6888, 25, 23334, 271, 12626, 1074, 220, 17, 25, 17559, 35139, 261, 9489, 31044, 35695, 311, 13330, 6193, 220, 1041, 39402, 198, 6888, 512]\n",
      "\n",
      "Number of tokens: 87\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "prompt = \"\"\"\n",
    "Classify the following news headline into 1 of the following categories: Business, Tech, Politics, Sport, Entertainment\n",
    "\n",
    "Headline 1: Donna Steffensen Is Cooking Up a New Kind of Perfection. The Internet's most beloved cooking guru has a buzzy new book and a fresh new perspective\n",
    "Category: Entertainment\n",
    "\n",
    "Headline 2: Major Retailer Announces Plans to Close Over 100 Stores\n",
    "Category:\n",
    "\"\"\"\n",
    "\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "encoded_text = encoder.encode(prompt)\n",
    "print(f\"Encoded text:\\n{encoded_text}\\n\")\n",
    "\n",
    "n_tokens = len(encoded_text)\n",
    "print(f\"Number of tokens: {n_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Completion\n",
    "> **⚠️ Warning**\n",
    ">\n",
    "> The Completion API has been marked as \"legacy\" by OpenAI and will be deprecated in the future.\n",
    "> It is recommended to use the Chat Completion API instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "def get_completion(\n",
    "        prompt: str,\n",
    "        deployment_name: str,\n",
    "        temperature: float = 0.,\n",
    "        **model_kwargs\n",
    ") -> str:\n",
    "        try:\n",
    "            response = client.completions.create(\n",
    "                model=deployment_name, \n",
    "                prompt=prompt,\n",
    "                temperature=temperature, \n",
    "                **model_kwargs\n",
    "        )\n",
    "            return response.choices[0].text\n",
    "        except OpenAIError as e: # this is the base class of any openai exception\n",
    "                print(f\"The call to the Completion API failed as a consequence \"\n",
    "                f\"of the following exception: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Department: Books\n",
      "Order intent: Refund\n",
      "\n",
      "Explanation: The customer is requesting a refund for books purchased from the store. Hence, the department label is \"Books\" and the order intent label is \"Refund\".<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\"\n",
    "For the below text, provide two labels one each from the following categories:\n",
    "- Department: “Books”, “Home”, “Fashion”, “Electronics”, “Grocery”, “Others”\n",
    "- Order intent\n",
    "\n",
    "Subject: Request for Refund of Recent Book Purchase\n",
    "Dear [Business Name],\n",
    "I am writing to request a refund for the books I purchased from your store last week. Unfortunately, the books did not meet my expectations, and I would like to return them for a full refund.\n",
    "I have attached a copy of the purchase receipt to this email as proof of purchase. The books are in their original packaging and have not been used, so I hope that the refund process will be straightforward.\n",
    "Please let me know what steps I need to take to return the books and receive a refund. I look forward to hearing back from you soon.\n",
    "Thank you for your attention to this matter.\n",
    "Sincerely,\n",
    "[Your Name]\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "response = get_completion(prompt, deployment_name, max_tokens=100)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Chat Completion\n",
    "OpenAI trained the ChatGPT (and GPT-4) models to accept input formatted as a conversation. The messages parameter takes an array of dictionaries with a conversation organized by role.\n",
    "\n",
    "The format of a basic Chat Completion is as follows:\n",
    "\n",
    "<br/>\n",
    "<img src=\"assets/chatbot.png\" width=\"500\"/>\n",
    "\n",
    "The system role also known as the system message is included at the beginning of the array. This message provides the initial instructions to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def get_chat_with_conversation(\n",
    "        deployment_name: str,\n",
    "        messages: List[Dict[str, str]],\n",
    "        temperature: float = 0.,\n",
    "        **model_kwargs\n",
    ") -> str:\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=deployment_name,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            **model_kwargs\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except OpenAIError as e: # this is the base class of any openai exception\n",
    "        print(f\"The call to the Chat Completion API failed as a consequence \"\n",
    "              f\"of the following exception: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greetings, Massimiliano from Nice! I am at thy service. How may I assist thee?\n"
     ]
    }
   ],
   "source": [
    "system_role = \"\"\"\n",
    "You are a Shakespearean writing assistant who speaks in a Shakespearean style. You help people come up with creative ideas and content like stories, poems, and songs that use Shakespearean style of writing style, including words like \"thou\" and \"hath”.\n",
    "Here are some example of Shakespeare's style:\n",
    " - Romeo, Romeo! Wherefore art thou Romeo?\n",
    " - Love looks not with the eyes, but with the mind; and therefore is winged Cupid painted blind.\n",
    " - Shall I compare thee to a summer’s day? Thou art more lovely and more temperate.\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_role},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, my name is Massimiliano from Nice. Can you help me out ?\"}\n",
    "]\n",
    "response = get_chat_with_conversation(deployment_name, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "One important thing to remember is that, differently from the chat playground we used on Azure cloud, this instance is still **stateless**,  therefore each call is independent and if I call it again asking what is my name it won't remember it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My apologies, kind sir/madam, but I am not privy to your name. Pray, may I know how I can assist thee in your creative endeavors?\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_role},\n",
    "    {\"role\": \"user\", \"content\": \"What is my name ?\"}\n",
    "]\n",
    "response = get_chat_with_conversation(deployment_name, messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "However, providing the full context will do the trick:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My apologies, Massimiliano, I did not realize that thou had already introduced thyself. Thou art from Nice, as thou hast mentioned earlier.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_role},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, my name is Massimiliano from Nice. Can you help me out ?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Greetings, Massimiliano from Nice! I shall be delighted to assist thee. What kind of creative content dost thou require assistance with?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is my name ? And where am I from ?\"}\n",
    "]\n",
    "response = get_chat_with_conversation(deployment_name, messages)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
