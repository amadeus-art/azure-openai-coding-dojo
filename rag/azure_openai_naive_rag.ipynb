{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/amadeus-art/azure-openai-coding-dojo/blob/main/azure_openai_qa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "# Question Answering on Documents using Azure OpenAI, Langchain and ChromaDB\n",
    "\n",
    "One of the main problems of Large Language Models (LLMs) is that they hallucinate (produce inaccurate or false information) when asked questions that are out of their scope. Also, their knowledge is up-to-date only if they are retrained or fine-tuned on recent data.\n",
    "\n",
    "In this tutorial we will showcase how to use the `langchain` library to perform question answering using \"knowledge base informed large language models\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install dependencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.1 langchain_openai python-dotenv chromadb -q"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment setup\n",
    "Before executing the following cells, make sure to set the following environment variables in the `.env` file or export them:\n",
    "* `AZURE_OPENAI_KEY`\n",
    "* `AZURE_OPENAI_ENDPOINT`\n",
    "* `MODEL_DEPLOYMENT_NAME`\n",
    "* `EMBEDDING_DEPLOYMENT_NAME`\n",
    "\n",
    "<br/>\n",
    "<img src=\"../assets/keys_endpoint.png\" width=\"800\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file\n",
    "\n",
    "openai_api_version = '2023-08-01-preview'  # latest as per today (15-09-2023), may change in the future\n",
    "\n",
    "# these are the name of the deployments you created in the Azure portal within the above resource\n",
    "model_deployment_name = os.getenv(\"MODEL_DEPLOYMENT_NAME\")\n",
    "embedding_deployment_name = os.getenv(\"EMBEDDING_DEPLOYMENT_NAME\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example of LLM response on a question after its knowledge cut"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, I cannot predict future events. The 2022 FIFA World Cup is scheduled to take place in Qatar from November 21 to December 18, 2022. The winner of the tournament will be determined at that time. \n",
      "\n",
      "As an AI language model, I cannot provide real-time information. The 2022 FIFA World Cup has not yet taken place, so the finalists are not known at this time. The tournament is scheduled to be held in Qatar from November 21 to December 18, 2022. The finalists will be determined through the qualification process, which is still ongoing.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    # key and endpoint are read from the .env file\n",
    "    openai_api_version=openai_api_version,\n",
    "    deployment_name=model_deployment_name,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "question_1 = HumanMessage(content=\"Who won the 2022 football world cup?\")\n",
    "question_2 = HumanMessage(content=\"List the finalists of the 2022 football world cup\")\n",
    "\n",
    "print(llm.invoke([question_1]).content, \"\\n\")\n",
    "print(llm.invoke([question_2]).content)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q&A on Docs\n",
    "The overall architecture of the Q&A on Docs is depicted below:\n",
    "<br/>\n",
    "<img src=\"../assets/qa_docs.png\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step. 1 - Load the document(s)\n",
    "Specify a `DocumentLoader` to load in your unstructured data as `Documents`. A `Document` is a piece of text (the page_content) and associated metadata."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "# NOTICE: this loader is not specifically designed for Wikipedia, it is just an example\n",
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/2022_FIFA_World_Cup\")\n",
    "data = loader.load()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step. 2 - Split\n",
    "Split the `Document` into chunks for embedding and vector storage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "print(len(all_splits))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step. 3 - Store\n",
    "To be able to look up our document splits, we first need to store them where we can later look them up. The most common way to do this is to embed the contents of each document then store the embedding and document in a vector store, with the embedding being used to index the document.\n",
    "\n",
    "NOTICE: Azure OpenAI embedding models currently only support batches of, at most, 16 chunks. For such reason, we set the `chunk_size` to 16."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embedding = AzureOpenAIEmbeddings(\n",
    "    # keys and endpoint are read from the .env file\n",
    "    openai_api_version=openai_api_version,\n",
    "    deployment=embedding_deployment_name,\n",
    ")\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=embedding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4. - Retrieve\n",
    "Retrieve relevant splits for any question using similarity search. By default, langchain retrieves the top 4 docs. Later on we will increase this number to increase the accuracy of the answer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "4"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Who won 2022 world cup\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 5. Generate\n",
    "Distill the retrieved documents into an answer using an LLM/Chat model with `RetrievalQA` chain.\n",
    "\n",
    "In this example we customized our prompt, for didactic purpose. This is however not mandatory.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_version=openai_api_version,\n",
    "    deployment_name=model_deployment_name,\n",
    "    temperature=0\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "'Argentina won the 2022 World Cup.'"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Don't try to make up the answer, only use the context to answer the question.\n",
    "The pieces of context refer to the Football World Cup 2022.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# by default, langchain retrieves the top 4 chunks, here we prefer to\n",
    "# retrieve more chunks to increase the chances of finding the answer\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 12})\n",
    "\n",
    "qa_chain = (\n",
    "    {\"context\": retriever,  \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Alternative (without LCEL):\n",
    "# qa_chain = RetrievalQA.from_chain_type(\n",
    "#     llm,\n",
    "#     retriever=vectorstore.as_retriever(search_kwargs={\"k\": 12}),\n",
    "#     chain_type_kwargs={\"prompt\": prompt},\n",
    "# )\n",
    "\n",
    "response = qa_chain.invoke(question)\n",
    "response"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "'In the final of the 2022 World Cup, Argentina and France played to a thrilling 3-3 draw after extra time. The match was eventually decided by a penalty shootout, with Argentina emerging as the champions with a 4-2 victory. Lionel Messi was named the best player of the tournament, while Kylian Mbapp√© finished as the top scorer with 8 goals.'"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.invoke(\"Summarize the final of the 2022 world cup with a few sentences with a lot of details\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "\"The opening ceremony of the World Cup 2022 took place on November 20, 2022, at the Al Bayt Stadium in Al Khor, Qatar. It featured appearances by Morgan Freeman and Ghanim Al-Muftah, as well as performances by South Korean singer Jungkook and Qatari singer Fahad Al Kubaisi. It was the first time that the Qur'an had been recited as part of the opening ceremony.\""
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.invoke(\"Tell me about the open ceremony of the world cup 2022\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "'The Netherlands played against the United States after the groups.'"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.invoke(\"Which team did the the Netherlands play after the groups?\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "'No, Italy did not participate in the tournament.'"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sad question ...\n",
    "qa_chain.invoke(\"How about Italy? Did they participate in the tournament?\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "'France did not win the final. Argentina won the final against France.'"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tricky question\n",
    "qa_chain.invoke(\"After France won the final, what happened?\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
