{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RAG with Langchain, Reranking and MergerRetriever (LOTR)\n",
    "\n",
    "This notebook discusses two enhancements of RAG:\n",
    "1. When you have different kind of documents, you can't create a single index. you have to create multiple indexes and have to index via all of them separately to perform the RAG. we are going to see how we can do a better RAG in this case using Merger Retriever (LOTR)\n",
    "\n",
    "2. When the context is long, we organize the documents as high-rank (high similarity) documents in top and bottom and put low / average documents in the middle using `LongContextReorder`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install dependencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain==0.1 langchain_openai python-dotenv chromadb huggingface_hub -q"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment setup\n",
    "Before executing the following cells, make sure to set the following environment variables in the `.env` file or export them:\n",
    "* `AZURE_OPENAI_KEY`\n",
    "* `AZURE_OPENAI_ENDPOINT`\n",
    "* `MODEL_DEPLOYMENT_NAME`\n",
    "* `EMBEDDING_DEPLOYMENT_NAME`\n",
    "* `COHERE_API_KEY` (create one [here](https://dashboard.cohere.com/api-keys))\n",
    "\n",
    "<br/>\n",
    "<img src=\"../assets/keys_endpoint.png\" width=\"800\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Merge Retriever - Lord of All the Retrievers (LOTR)\n",
    "\n",
    "Lord of the Retrievers, also known as `MergerRetriever`, takes a list of retrievers as input and merges the results of their `get_relevant_documents()` methods into a single list. The merged results will be a list of documents that are relevant to the query and that have been ranked by the different retrievers.\n",
    "\n",
    "The `MergerRetriever` class can be used to improve the accuracy of document retrieval in a number of ways.\n",
    "\n",
    "First, it can combine the results of multiple retrievers, which can help to reduce the risk of bias in the results. Second, it can rank the results of the different retrievers, which can help to ensure that the most relevant documents are returned first."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "\n",
    "openai_api_version = \"2023-08-01-preview\"\n",
    "\n",
    "openai_embedding = AzureOpenAIEmbeddings(\n",
    "    deployment=os.getenv('EMBEDDING_DEPLOYMENT_NAME'),\n",
    "    openai_api_version=openai_api_version,\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "hf_bge_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=\"BAAI/bge-large-en\",\n",
    "    model_kwargs={\"device\": device},\n",
    "    encode_kwargs = {'normalize_embeddings': False}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the Data Stores\n",
    "We will be using two data sources:\n",
    "- langchain documentation, scraped in the following cell\n",
    "- langchain blogposts (same used in the previous notebook)\n",
    "\n",
    "The first one will be embedded using text-embeddings-ada-002 (OpenAI), while the second one with an open-source model (`bge-large-en`)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from langchain_docs_loader import load_langchain_docs_splitted\n",
    "# This might be slow the very first time you run it. Hang in there!\n",
    "langchain_docs = load_langchain_docs_splitted()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3139\n"
     ]
    }
   ],
   "source": [
    "print(len(langchain_docs))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### First retriever"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "persist_directory = '../data/embeddings_aoi_docs/'\n",
    "\n",
    "# This is going to be slow the first time you run it (it is embedding 3139 chunks, in batches of 16).\n",
    "if not os.path.exists(persist_directory):\n",
    "    vectordb_aoi = Chroma.from_documents(\n",
    "        documents=langchain_docs,\n",
    "        embedding=openai_embedding,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    vectordb_aoi.persist()\n",
    "else:\n",
    "    # load from disk\n",
    "    vectordb_aoi = Chroma(\n",
    "        embedding_function=openai_embedding,\n",
    "        persist_directory=persist_directory\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Second retriever"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "text_loader_kwargs={'autodetect_encoding': True}\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    '../data/langchain_blog_posts/',\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs=text_loader_kwargs,\n",
    "    #silent_errors=True\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "#splitting the text into\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "429"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "persist_directory = '../data/embeddings_hf_bge_docs/'\n",
    "\n",
    "# This is going to be slow the first time you run it, unless you have a GPU supporting CUDA.\n",
    "# This is encoding the same texts used for the reranking notebook, but with a different embedding model.\n",
    "if not os.path.exists(persist_directory):\n",
    "    vectordb_hf = Chroma.from_documents(\n",
    "        documents=texts,\n",
    "        embedding=hf_bge_embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    vectordb_hf.persist()\n",
    "else:\n",
    "    # load from disk\n",
    "    vectordb_hf = Chroma(\n",
    "        embedding_function=hf_bge_embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Merge the embeddings of the documents\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from langchain.retrievers import MergerRetriever\n",
    "\n",
    "n_docs = 10\n",
    "retriever_aoi = vectordb_aoi.as_retriever(\n",
    "    search_type = \"similarity\",\n",
    "    search_kwargs = {\"k\": n_docs}\n",
    ")\n",
    "\n",
    "retriever_hf = vectordb_hf.as_retriever(\n",
    "    search_type = \"similarity\",\n",
    "    search_kwargs = {\"k\": n_docs}\n",
    ")\n",
    "lotr = MergerRetriever(retrievers=[retriever_aoi, retriever_hf])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query = \"What is the difference between a MergerRetriever and EnsembleRetriever in langchain?\"\n",
    "docs = lotr.get_relevant_documents(query)\n",
    "docs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Remove duplicate information and reorder with LITM (Lost in the Middle).\n",
    "\n",
    "After merging the indexes, we perform the following three transformations:\n",
    "- Remove duplicate information\n",
    "- Reorder the results with `LongContextReorder`, which implements LITM (Lost in the Middle)[^1]\n",
    "- Rerank the results with Cohere ranker.\n",
    "\n",
    "[^1]: LITM is a method for reordering the results of a search engine to improve the relevance of the results. It is based on the idea that the most relevant results are likely to be found in the middle of the list of results, rather than at the beginning or the end. The method works by reordering the results so that the most relevant results are in the middle of the list, and the least relevant results are at the beginning and the end. ([paper](https://arxiv.org/abs//2307.03172))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter, LongContextReorder\n",
    "\n",
    "# We can remove redundant results from both retrievers using yet another embedding.\n",
    "# Using multiples embeddings in diff steps could help reduce biases.\n",
    "filter_emb = EmbeddingsRedundantFilter(embeddings=openai_embedding)\n",
    "reorder = LongContextReorder()\n",
    "\n",
    "pipeline = DocumentCompressorPipeline(transformers=[filter_emb, reorder])\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=pipeline, base_retriever=lotr\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question-Answering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=os.getenv('MODEL_DEPLOYMENT_NAME'),\n",
    "    openai_api_version=openai_api_version,\n",
    "    temperature=0.,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "\n",
    "qa_endpoint = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=compression_retriever\n",
    ")\n",
    "\n",
    "template = \"\"\"User question: {question}\n",
    "\n",
    "possible Answer: Provide the best answer refering to the context.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new LLMChain chain...\u001B[0m\n",
      "Prompt after formatting:\n",
      "\u001B[32;1m\u001B[1;3mUser question: What is the difference between a MergerRetriever and EnsembleRetriever in langchain?\n",
      "\n",
      "possible Answer: Provide the best answer refering to the context.\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "{'question': 'What is the difference between a MergerRetriever and EnsembleRetriever in langchain?',\n 'text': \"In Langchain, a MergerRetriever and EnsembleRetriever are both retrievers used for information retrieval tasks, but they have different functionalities.\\n\\nA MergerRetriever is a retriever that combines multiple retrievers into a single retriever. It takes the outputs of multiple retrievers and merges them to provide a unified set of results. This can be useful when you want to leverage the strengths of different retrievers and get a more comprehensive set of information.\\n\\nOn the other hand, an EnsembleRetriever is a retriever that combines multiple retrievers by aggregating their scores. Instead of merging the results, it assigns weights to each retriever's scores and combines them to produce a final ranking. This approach allows you to benefit from the different strengths of each retriever while still maintaining a single ranked list of results.\\n\\nIn summary, a MergerRetriever combines the results of multiple retrievers, while an EnsembleRetriever combines the scores of multiple retrievers. The choice between the two depends on the specific requirements of your information retrieval task.\"}"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(query)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
