{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Advanced RAG with Langchain and Rerankers\n",
    "\n",
    "In this notebook we will enhance the previous \"naive RAG\" using a **re-ranking model**.\n",
    "\n",
    "## Environment setup\n",
    "Before executing the following cells, make sure to set the following environment variables in the `.env` file or export them:\n",
    "* `AZURE_OPENAI_KEY`\n",
    "* `AZURE_OPENAI_ENDPOINT`\n",
    "* `MODEL_DEPLOYMENT_NAME`\n",
    "* `EMBEDDING_DEPLOYMENT_NAME`\n",
    "* `COHERE_API_KEY` (create one [here](https://dashboard.cohere.com/api-keys))\n",
    "\n",
    "<br/>\n",
    "<img src=\"../assets/keys_endpoint.png\" width=\"800\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain==0.1.0 cohere python-dotenv langchain_openai"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reranking\n",
    "A Reranker is a supervised model trained using a dataset of pairs (query, tag) created via self-supervision such that, given a query and document pair, will output a similarity score. We use this score to reorder the documents by relevance to our query, thus creating a two-stage retrieval system. A first-stage model (an embedding model/retriever) retrieves a set of relevant documents from a larger dataset. Then, a second-stage model (the reranker) is used to rerank those documents retrieved by the first-stage model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br/>\n",
    "<img src=\"../assets/reranking.png\" width=\"800\"/>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "openai_api_version = \"2023-08-01-preview\"\n",
    "\n",
    "embedder = AzureOpenAIEmbeddings(\n",
    "    deployment=os.getenv('EMBEDDING_DEPLOYMENT_NAME'),\n",
    "    openai_api_version=openai_api_version,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this notebook, we've scraped langchain's blogs (see `data/`)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "429"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_loader_kwargs={'autodetect_encoding': True}\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    '../data/langchain_blog_posts/',\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs=text_loader_kwargs,\n",
    "    #silent_errors=True\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "#splitting the text into\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "len(texts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "persist_directory = '../data/embeddings/'\n",
    "\n",
    "if not os.path.exists(persist_directory):\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=texts,\n",
    "        embedding=embedder,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    vectordb.persist()\n",
    "else:\n",
    "    # load from disk\n",
    "    vectordb = Chroma(\n",
    "        embedding_function=embedder,\n",
    "        persist_directory=persist_directory\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "query = \"What is Langsmith?\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the approach described in the previous notebook, we retrieve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "25"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 25})\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "len(docs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "cohere_client = CohereRerank(\n",
    "    model=\"rerank-english-v2.0\",\n",
    "    top_n=5,\n",
    ") # api key read from .env"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def compare(query: str):\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    docs_content = [doc.page_content for doc in docs]\n",
    "    id2doc = {idx : doc for idx, doc in enumerate(docs_content)}\n",
    "    doc2id = {doc : idx for idx, doc in enumerate(docs_content)}\n",
    "\n",
    "    rerank_docs = cohere_client.compress_documents(\n",
    "        query=query,\n",
    "        documents=docs\n",
    "    )\n",
    "    original_docs = []\n",
    "    reranked_docs = []\n",
    "    # compare order change\n",
    "    for i, doc in enumerate(rerank_docs):\n",
    "        rerank_i = doc2id[doc.page_content]\n",
    "        print(f\"{i}\\t->\\t{rerank_i}\")\n",
    "        if i != rerank_i:\n",
    "            reranked_docs.append(f\"[{rerank_i}]\\n{doc.page_content}\")\n",
    "            original_docs.append(f\"[{i}]\\n{id2doc[i]}\")\n",
    "\n",
    "    for orig, rerank in zip(original_docs, reranked_docs):\n",
    "        print(f\"ORIGINAL:\\n{orig}\\n\\nRE-RANKED:\\n{rerank}\\n\\n---\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t->\t0\n",
      "1\t->\t1\n",
      "2\t->\t14\n",
      "3\t->\t2\n",
      "4\t->\t22\n",
      "ORIGINAL:\n",
      "[2]\n",
      "\"LangChain's (the company's) goal is to make it as easy as possible to develop LLM applications\"\n",
      "\n",
      "said Harrison Chase, co-founder and CEO of LangChain.\n",
      "\n",
      "\"To that end, we realized pretty early that what was needed - and missing - wasn't just an open source tool like LangChain, but also a complementary platform for managing these new types of applications. To that end, we built LangSmith - which is usable with or without LangChain and let's users easily debug, monitor, test, evaluate, and now (with the recently launched Hub) share and collaborate on their LLM applications.”\n",
      "\n",
      "\n",
      "\n",
      "What Are LangSmith Traces?\n",
      "\n",
      "RE-RANKED:\n",
      "[14]\n",
      "As a very simple example, we considered it to be table stakes for LangSmith to help users easily create datasets from existing logs and use them immediately for testing and evaluation, seamlessly connecting the logging/debugging workflows to the testing/evaluation ones.\n",
      "\n",
      "\n",
      "\n",
      "Fintual, a Latin American startup with big dreams to help their citizens build wealth through a personalized financial advisor, found LangSmith early in their LLM development journey. “As soon as we heard about LangSmith, we moved our entire development stack onto it. We could build the evaluation, testing, and monitoring tools we needed in-house. But it would be 1000x worse, take us 10x longer, and require a team 2x the size,” said Fintual leader Jose Pena.\n",
      "\n",
      "“Because we are building financial products, the bar for accuracy, personalization, and security is particularly high. LangSmith helps us build products we are confident putting in front of users.”\n",
      "\n",
      "---\n",
      "\n",
      "ORIGINAL:\n",
      "[3]\n",
      "https://twitter.com/zhanghaili0610\n",
      "\n",
      "LangChain vs LangSmith: What’s the difference?\n",
      "\n",
      "While LangChain is the muscle doing the heavy lifting with Chains, Prompts, and Agents, understanding the 'why' behind the decisions LLMs make is a maze we often found ourselves lost in. That's where LangSmith shines, acting as an AI compass built into LangChain, guiding us through the intricate decision pathways and results that our chatbot generates.\n",
      "\n",
      "\"LangChain's (the company's) goal is to make it as easy as possible to develop LLM applications\"\n",
      "\n",
      "said Harrison Chase, co-founder and CEO of LangChain.\n",
      "\n",
      "RE-RANKED:\n",
      "[2]\n",
      "\"LangChain's (the company's) goal is to make it as easy as possible to develop LLM applications\"\n",
      "\n",
      "said Harrison Chase, co-founder and CEO of LangChain.\n",
      "\n",
      "\"To that end, we realized pretty early that what was needed - and missing - wasn't just an open source tool like LangChain, but also a complementary platform for managing these new types of applications. To that end, we built LangSmith - which is usable with or without LangChain and let's users easily debug, monitor, test, evaluate, and now (with the recently launched Hub) share and collaborate on their LLM applications.”\n",
      "\n",
      "\n",
      "\n",
      "What Are LangSmith Traces?\n",
      "\n",
      "---\n",
      "\n",
      "ORIGINAL:\n",
      "[4]\n",
      "The blocker has now changed. While it’s easy to build a prototype of an application in ~5 lines of LangChain code, it’s still deceptively hard to take an application from prototype to production. The main issue that we see today is application performance–something that works ~30% of the time is good enough for a Twitter demo, but not nearly good enough for production.\n",
      "\n",
      "Today, we’re introducing LangSmith, a platform to help developers close the gap between prototype and production. It’s designed for building and iterating on products that can harness the power–and wrangle the complexity–of LLMs.\n",
      "\n",
      "LangSmith is now in closed beta. So if you’re looking for a robust, unified, system for debugging, testing, evaluating, and monitoring your LLM applications, sign up here.\n",
      "\n",
      "\n",
      "\n",
      "How did we get here?\n",
      "\n",
      "RE-RANKED:\n",
      "[22]\n",
      "We (RealChar team) are pleased to share our experience using LangSmith and working with LangChain team.\n",
      "\n",
      "In case you donâ€™t know, RealChar is an open source project to let you create, customize and talk to your AI character/companion in realtime (all in one codebase). We offer users natural and seamless conversations with AI on all the common platforms (mobile, web, terminal and desktop soon). We built RealChar leveraging some of best open source tools in the Generative AI/LLM space, including LangChain.\n",
      "\n",
      "Just a fun demo: asking AI Elon about whether he is afraid of losing in the much anticipated cage fight. Full version here.\n",
      "\n",
      "RealChar received a ton of attention and usage from the community after releasing it just a week ago, and our site has undergo significant traffic. With conversations piling up and logs get cluttered very quickly, we found LangSmith to be a perfect tool for us to monitor and observe the traffic.\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Langsmith?\"\n",
    "compare(query)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t->\t2\n",
      "1\t->\t11\n",
      "2\t->\t1\n",
      "3\t->\t0\n",
      "4\t->\t12\n",
      "ORIGINAL:\n",
      "[0]\n",
      "The LangChain library has multiple SQL chains and even an SQL agent aimed at making interacting with data stored in SQL as easy as possible. Here are some relevant links:\n",
      "\n",
      "Introduction\n",
      "\n",
      "Most of an enterprise’s data is traditionally stored in SQL databases. With the amount of valuable data stored there, business intelligence (BI) tools that make it easy to query and understand the data present there have risen in popularity. But what if you could just interact with a SQL database in natural language? With LLMs today, that is possible. LLMs have an understanding of SQL and are able to write it pretty well. However, there are several issues that make this a non-trivial task.\n",
      "\n",
      "The Problems\n",
      "\n",
      "So LLMs can write SQL - what more is needed?\n",
      "\n",
      "Unfortunately, a few things.\n",
      "\n",
      "RE-RANKED:\n",
      "[2]\n",
      "URL: https://blog.langchain.dev/llms-and-sql/\n",
      "Title: LLMs and SQL\n",
      "\n",
      "Francisco Ingham and Jon Luo are two of the community members leading the change on the SQL integrations. We’re really excited to write this blog post with them going over all the tips and tricks they’ve learned doing so. We’re even more excited to announce that we’ll be doing an hour long webinar with them to discuss these learnings and field other related questions. This webinar will be on March 22nd - sign up at the below link:\n",
      "\n",
      "The LangChain library has multiple SQL chains and even an SQL agent aimed at making interacting with data stored in SQL as easy as possible. Here are some relevant links:\n",
      "\n",
      "Introduction\n",
      "\n",
      "---\n",
      "\n",
      "ORIGINAL:\n",
      "[1]\n",
      "The LangChain library provides different tools to interact with SQL databases which can be used to build and run queries based on natural language inputs. For example, the standard SQL Toolkit draws from standard best practices that have been extensively covered in this blogpost. However, there is still room for improvement when it comes to building a custom solution and adjusting the generic tools to the specific use case. The advantage of having a plug and play toolkit contrasts with having a solution that is not flexible enough for the user to incorporate their domain-specific knowledge about the databases.\n",
      "\n",
      "RE-RANKED:\n",
      "[11]\n",
      "For more in-depth examples, head to: https://python.langchain.com/docs/modules/data_connection/indexing\n",
      "\n",
      "ChatLangChain + Indexing API\n",
      "\n",
      "Weâ€™ve recently revamped the https://github.com/langchain-ai/chat-langchain chatbot for questions about LangChain. As part of the revamp, we revived the hosted version https://chat.langchain.com and set up a daily indexing job using the new API to make sure the chatbot is up to date with the latest LangChain developments.\n",
      "\n",
      "Doing this was very straightforward â€” all we had to do was:\n",
      "\n",
      "Set up a Supabase Postgres database to be used as a record manager, Update our ingestion script to use the indexing API instead of inserting documents to the vector store directly, Set up a scheduled Github Action to run the ingestion script daily. You can check out the GHA workflow here.\n",
      "\n",
      "Conclusion\n",
      "\n",
      "---\n",
      "\n",
      "ORIGINAL:\n",
      "[2]\n",
      "URL: https://blog.langchain.dev/llms-and-sql/\n",
      "Title: LLMs and SQL\n",
      "\n",
      "Francisco Ingham and Jon Luo are two of the community members leading the change on the SQL integrations. We’re really excited to write this blog post with them going over all the tips and tricks they’ve learned doing so. We’re even more excited to announce that we’ll be doing an hour long webinar with them to discuss these learnings and field other related questions. This webinar will be on March 22nd - sign up at the below link:\n",
      "\n",
      "The LangChain library has multiple SQL chains and even an SQL agent aimed at making interacting with data stored in SQL as easy as possible. Here are some relevant links:\n",
      "\n",
      "Introduction\n",
      "\n",
      "RE-RANKED:\n",
      "[1]\n",
      "The LangChain library provides different tools to interact with SQL databases which can be used to build and run queries based on natural language inputs. For example, the standard SQL Toolkit draws from standard best practices that have been extensively covered in this blogpost. However, there is still room for improvement when it comes to building a custom solution and adjusting the generic tools to the specific use case. The advantage of having a plug and play toolkit contrasts with having a solution that is not flexible enough for the user to incorporate their domain-specific knowledge about the databases.\n",
      "\n",
      "---\n",
      "\n",
      "ORIGINAL:\n",
      "[3]\n",
      "LangChain Expression Language creates chains that integrate seamlessly with LangSmith. Here is a trace for the above:\n",
      "\n",
      "You can inspect the trace here. Previously, when creating a custom chain there was actually a good bit of work to be done to make sure callbacks were passed through correctly so that it could be traced correctly. With LangChain Expression Language that happens automatically.\n",
      "\n",
      "We've also tried to make this as easy as possible for people to learn by creating a \"LangChain Teacher\" application that will walk you through the basics of getting started with LangChain Expression Language. You can access it here. We'll be open sourcing this soon.\n",
      "\n",
      "We'll also be doing a webinar on this tomorrow. We'll cover the standard interface it exposes, how to use it, and why to use it. Register for that here.\n",
      "\n",
      "RE-RANKED:\n",
      "[0]\n",
      "The LangChain library has multiple SQL chains and even an SQL agent aimed at making interacting with data stored in SQL as easy as possible. Here are some relevant links:\n",
      "\n",
      "Introduction\n",
      "\n",
      "Most of an enterprise’s data is traditionally stored in SQL databases. With the amount of valuable data stored there, business intelligence (BI) tools that make it easy to query and understand the data present there have risen in popularity. But what if you could just interact with a SQL database in natural language? With LLMs today, that is possible. LLMs have an understanding of SQL and are able to write it pretty well. However, there are several issues that make this a non-trivial task.\n",
      "\n",
      "The Problems\n",
      "\n",
      "So LLMs can write SQL - what more is needed?\n",
      "\n",
      "Unfortunately, a few things.\n",
      "\n",
      "---\n",
      "\n",
      "ORIGINAL:\n",
      "[4]\n",
      "LANGCHAIN_TRACING_V2=true LANGCHAIN_ENDPOINT=https://api.smith.langchain.com LANGCHAIN_API_KEY=YOUR_LANGCHAIN_API_KEY LANGCHAIN_PROJECT=YOUR_LANGCHAIN_PROJECT\n",
      "\n",
      "Overall, we see LangSmith as a great tool for Analytics, Observability, and Evaluation, all in one place. Itâ€™s very useful for a production-level application with large volume of traffic like RealChar.\n",
      "\n",
      "/content/media/5101573/253656635-5de0b023-6cf3-4947-84cb-596f429d109e.mp4\n",
      "\n",
      "RE-RANKED:\n",
      "[12]\n",
      "We discuss each of these solutions in a separate section below.\n",
      "\n",
      "Describing your database\n",
      "\n",
      "To provide the LLM with enough information for it to generate reasonable queries for a given database, we need to effectively describe the database in the prompt. This can include describing table structure, examples of what the data looks like, and even examples of good queries for the database. The examples below come from the Chinook database.\n",
      "\n",
      "Describing the schema\n",
      "\n",
      "In older versions of LangChain, we simply provided the table names, columns, and their types:\n",
      "\n",
      "Table 'Track' has columns: TrackId (INTEGER), Name (NVARCHAR(200)), AlbumId (INTEGER), MediaTypeId (INTEGER), GenreId (INTEGER), Composer (NVARCHAR(220)), Milliseconds (INTEGER), Bytes (INTEGER), UnitPrice (NUMERIC(10, 2))\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare(\"Explain how to connect langchain to mysql\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question-answering using Reranking\n",
    "A comparison between the naive approach against re-ranking."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LangChain library provides tools to interact with SQL databases, including SQL chains and an SQL agent. However, the specific code to connect LangChain to SQL is not mentioned in the given context.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=os.getenv('MODEL_DEPLOYMENT_NAME'),\n",
    "    openai_api_version=openai_api_version,\n",
    "    temperature=0.,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Don't try to make up the answer, only use the context to answer the question.\n",
    "The pieces of context refer to langchain.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "qa_chain_base = (\n",
    "    RunnableParallel(\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "question = \"Explain how to connect langchain to sql. Show me the code to do that\"\n",
    "print(qa_chain_base.invoke(question))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To connect LangChain to SQL, you can use the following code:\n",
      "\n",
      "```\n",
      "from langchain.agents import AgentExecutor, create_sql_agent\n",
      "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
      "from langchain.agents.agent_types import AgentType\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.llms.openai import OpenAI\n",
      "from langchain.sql_database import SQLDatabase\n",
      "\n",
      "def create_agent(db_uri, agent_type=AgentType.OPENAI_FUNCTIONS, verbose=VERBOSE_LANGCHAIN, temperature=0, model=\"gpt-3.5-turbo-0613\"):\n",
      "    db = SQLDatabase.from_uri(db_uri)\n",
      "    toolkit = SQLDatabaseToolkit(db=db, llm=OpenAI(temperature=temperature))\n",
      "    return create_sql_agent(\n",
      "        llm=ChatOpenAI(temperature=temperature, model=model),\n",
      "        toolkit=toolkit,\n",
      "        verbose=verbose,\n",
      "        agent_type=agent_type,\n",
      "    )\n",
      "```\n",
      "\n",
      "This code initializes the LangChain Agent and connects it to your SQL database. You need to provide the `db_uri` parameter with the URI of your SQL database.\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 30})\n",
    "compressor = CohereRerank(top_n=3)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")\n",
    "\n",
    "qa_chain_rerank = (\n",
    "    RunnableParallel(\n",
    "        {\"context\": compression_retriever, \"question\": RunnablePassthrough()}\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(qa_chain_rerank.invoke(question))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extra #1: Open-source Rerankers\n",
    "\n",
    "Cohere's reranker is a propietary model, for which you need an access token and a paid subscription for production application. However, there are open-source alternatives that you can use for free.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q flashrank"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing import List\n",
    "from flashrank import Ranker, RerankRequest\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache\n",
    "def get_ranker(model_name: str = \"ms-marco-MultiBERT-L-12\"):\n",
    "    return Ranker(model_name=model_name)\n",
    "\n",
    "def rerank_with_multiBERT(\n",
    "        query: str,\n",
    "        docs: List[Document],\n",
    "        top_n: int = 5,\n",
    "):\n",
    "    reranker = get_ranker()\n",
    "    passages = [{\"id\": i, \"text\": doc.page_content} for i, doc in enumerate(docs)]\n",
    "\n",
    "    rerank_request = RerankRequest(\n",
    "        query=query,\n",
    "        passages=passages\n",
    "    )\n",
    "    rerank_response = reranker.rerank(rerank_request)\n",
    "    return rerank_response[:top_n]\n",
    "\n",
    "\n",
    "query = \"What is Langsmith?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "rerank_docs = rerank_with_multiBERT(query, docs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'id': 27,\n  'text': \"What Are LangSmith Traces?\\n\\nTraces in the world of LangSmith are analogous to logs when programming; they allow us to easily see what text came in and out of chains and LLMs. Think of them as detailed breadcrumbs illuminating the AI's journey. Each trace, like a footprint on a sandy beach, represents a pivotal AI decision. Traces don't merely depict the path taken; they shed light on the underlying thought process and actions taken at each juncture.\\n\\nHere’s what one of our traces looks like inside LangSmith:\\n\\nAll the individual traces are consolidated into datasets:\\n\\n\\n\\nDo You Really Need To Use LangSmith?\\n\\nWhen generative AI works, it feels like watching a viral “satisfying video” montage - so delightful. But when it doesn’t, it sucks, and sometimes it sucks real bad.\",\n  'score': 0.9682064},\n {'id': 0,\n  'text': 'Since the launch of HelpHub, we were trying to do things on hard mode when it came to iterating and improving functionality. That is, of course, until the LangChain team tantalized us onto their LangSmith beta. What we didn’t expect was how immediate the downstream improvements were to our flagship AI-powered product.\\n\\nBut is LangSmith robust enough for us to rely on entirely for our LLM-powered QA? Or is it just another nice-to-have feature for our ENG team?\\n\\nIf you’re at the intersection of product, LLMs, and user experience, we’ve just walked so you can run. Time to read on.\\n\\nWhat Is LangSmith?\\n\\nLangSmith is a framework built on the shoulders of LangChain. It’s designed to track the inner workings of LLMs and AI agents within your product.\\n\\nThose LLM inner-workings can be categorized into 4 main buckets - each with its own flair of usefulness. Here’s a breakdown of how they all work in unison and what you can expect.\\n\\n\\n\\nDebugging:',\n  'score': 0.96282333},\n {'id': 3,\n  'text': 'https://twitter.com/zhanghaili0610\\n\\nLangChain vs LangSmith: What’s the difference?\\n\\nWhile LangChain is the muscle doing the heavy lifting with Chains, Prompts, and Agents, understanding the \\'why\\' behind the decisions LLMs make is a maze we often found ourselves lost in. That\\'s where LangSmith shines, acting as an AI compass built into LangChain, guiding us through the intricate decision pathways and results that our chatbot generates.\\n\\n\"LangChain\\'s (the company\\'s) goal is to make it as easy as possible to develop LLM applications\"\\n\\nsaid Harrison Chase, co-founder and CEO of LangChain.',\n  'score': 0.95826834},\n {'id': 20,\n  'text': \"Those LLM inner-workings can be categorized into 4 main buckets - each with its own flair of usefulness. Here’s a breakdown of how they all work in unison and what you can expect.\\n\\n\\n\\nDebugging:\\n\\nWhen your LLM starts throwing curveballs instead of answers, you don't just want to sit there catching them. With LangSmith, you can roll up your sleeves and play detective. We use the debugging tools to dive into perplexing agent loops, frustratingly slow chains, and to scrutinize prompts like they're suspects in a lineup.\\n\\nTesting:\\n\\nTesting LLM applications without LangSmith is like trying to assemble IKEA furniture without the manual: sure, you could wing it, but do you really want to risk it? Baked into LangSmith is the option to utilize existing datasets or create new ones, and run them against your chains. Visual feedback on outputs and accuracy metrics are presented within the interface, streamlining the testing process for our eng team (we really like this).\\n\\nEvaluating:\",\n  'score': 0.9479166},\n {'id': 2,\n  'text': '\"LangChain\\'s (the company\\'s) goal is to make it as easy as possible to develop LLM applications\"\\n\\nsaid Harrison Chase, co-founder and CEO of LangChain.\\n\\n\"To that end, we realized pretty early that what was needed - and missing - wasn\\'t just an open source tool like LangChain, but also a complementary platform for managing these new types of applications. To that end, we built LangSmith - which is usable with or without LangChain and let\\'s users easily debug, monitor, test, evaluate, and now (with the recently launched Hub) share and collaborate on their LLM applications.”\\n\\n\\n\\nWhat Are LangSmith Traces?',\n  'score': 0.93474686}]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerank_docs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extra #2: Rerank with a LLM\n",
    "We can also use a LLM to rerank the documents, asking it to predict the relevance of the document to the question. This approach is likely to be less effective than using a model designed for this task, but it is interesting to see how it works."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import PydanticOutputFunctionsParser\n",
    "from langchain_core.utils.function_calling import convert_pydantic_to_openai_function\n",
    "from pydantic import Field\n",
    "from pydantic import BaseModel\n",
    "\n",
    "map_prompt_str = \"\"\"\n",
    "\n",
    "Context of the task : Technical documents related to langchain.\n",
    "Copy the parts of the following document that are related to the statement: \"{question}\".\n",
    "\n",
    "==== Document : ====\n",
    "{context}\n",
    "==== End of Document ====\n",
    "\"\"\"\n",
    "\n",
    "def get_rerank_chain(\n",
    "        llm,\n",
    "        doc_acceptance_threshold: float = 0.5,\n",
    "        max_rerank_docs: int = 4\n",
    "):\n",
    "    map_prompt = PromptTemplate(template=map_prompt_str, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "    class AnalysisAndContextEvaluation(BaseModel):\n",
    "        \"\"\"Return the answer to the question and a relevance score.\"\"\"\n",
    "\n",
    "        analysis: str = Field(description=\"How can the context help to answer the question?\")\n",
    "        evaluation: str = Field(\n",
    "            description=\"One word, chosen among these adjectives: perfect, high, good, ok, irrelevant.\"\n",
    "        )\n",
    "        score: float = Field(\n",
    "            description=\"A 0.0-1.0 relevance score, where 1.0 indicates the provided context answers the question completely and 0.0 indicates the provided context does not answer the question at all.\"\n",
    "        )\n",
    "\n",
    "    function = convert_pydantic_to_openai_function(AnalysisAndContextEvaluation)\n",
    "    chain_rerank_doc = (\n",
    "        map_prompt\n",
    "        | llm.bind(\n",
    "            temperature=0, functions=[function], function_call={\"name\": \"AnalysisAndContextEvaluation\"}\n",
    "        )\n",
    "        | PydanticOutputFunctionsParser(pydantic_schema=AnalysisAndContextEvaluation)\n",
    "    ).with_config(run_name=\"Rerank\")\n",
    "\n",
    "    \"\"\"\n",
    "    here we define the mapping chain:\n",
    "    chain_map_docs (takes into input a question 'question', and the documents list: 'documents')\n",
    "    \"\"\"\n",
    "\n",
    "    def chain_get_context_and_questions(__input):\n",
    "        question = __input[\"question\"]\n",
    "        return [{\"question\": question, \"context\": d.page_content} for d in __input[\"documents\"]]\n",
    "\n",
    "    chain_docs_and_ranks = RunnableParallel(\n",
    "        documents=lambda x: x[\"documents\"],\n",
    "        ranks=chain_get_context_and_questions | (chain_rerank_doc | (lambda x: x.score)).map(),\n",
    "    )\n",
    "\n",
    "    def keep_best_docs(__input):\n",
    "        float_ranks = __input[\"ranks\"]\n",
    "        docs_sorted = [\n",
    "            x[0]\n",
    "            for x in sorted(zip(__input[\"documents\"], float_ranks), key=lambda x: x[1], reverse=True)\n",
    "            if x[1] >= doc_acceptance_threshold\n",
    "        ]\n",
    "        return docs_sorted[0:max_rerank_docs]\n",
    "\n",
    "    # the mapper chain\n",
    "    chain_rerank_docs = (chain_docs_and_ranks | keep_best_docs).with_config(\n",
    "        run_name=\"chain_map_docs: Rerank docs\"\n",
    "    )\n",
    "\n",
    "    return chain_rerank_docs\n",
    "\n",
    "\n",
    "rerank_chain_llm = get_rerank_chain(llm, max_rerank_docs=5)\n",
    "qa_chain_llm = (\n",
    "        RunnableParallel(question=RunnablePassthrough(), documents=retriever)\n",
    "        | rerank_chain_llm\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ranked_docs_llm = qa_chain_llm.invoke(question)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
