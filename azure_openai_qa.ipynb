{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/amadeus-art/azure-openai-coding-dojo/blob/main/azure_openai_qa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "# Question Answering on Documents using Azure OpenAI, Langchain and ChromaDB\n",
    "\n",
    "One of the main problems of Large Language Models (LLMs) is that they hallucinate (produce inaccurate or false information) when asked questions that are out of their scope. Also, their knowledge is up-to-date only if they are retrained or fine-tuned on recent data.\n",
    "\n",
    "In this tutorial we will showcase how to use the `langchain` library to perform question answering using \"knowledge base informed large language models\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install dependencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "!pip install openai langchain python-dotenv chromadb -q"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment setup\n",
    "Before executing the following cells, make sure to set the `AZURE_OPENAI_KEY` and `AZURE_OPENAI_ENDPOINT` variables in the `.env` file or export them.\n",
    "\n",
    "<br/>\n",
    "<img src=\"assets/keys_endpoint.png\" width=\"800\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file\n",
    "\n",
    "openai_api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "openai_api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  # should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/\n",
    "openai_api_type = 'azure'\n",
    "openai_api_version = '2023-07-01-preview'  # latest as per today (15-09-2023), may change in the future\n",
    "\n",
    "# these are the name of the deployments you created in the Azure portal within the above resource\n",
    "model_deployment_name = os.getenv(\"MODEL_DEPLOYMENT_NAME\")\n",
    "embedding_deployment_name = os.getenv(\"EMBEDDING_DEPLOYMENT_NAME\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example of LLM response on a question after its knowledge cut"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, I cannot predict the future. The 2022 football world cup has not yet taken place. It is scheduled to be held in Qatar from November 21 to December 18, 2022. \n",
      "\n",
      "As an AI language model, I don't have access to future events. The finalists of the 2022 football world cup are yet to be determined.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_api_base=openai_api_base,\n",
    "    openai_api_type=openai_api_type,\n",
    "    openai_api_version=openai_api_version,\n",
    "    deployment_name=model_deployment_name,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "question_1 = HumanMessage(content=\"Who won the 2022 football world cup?\")\n",
    "question_2 = HumanMessage(content=\"List the finalists of the 2022 football world cup\")\n",
    "\n",
    "print(llm([question_1]).content, \"\\n\")\n",
    "print(llm([question_2]).content)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Q&A on Docs\n",
    "The overall architecture of the Q&A on Docs is depicted below:\n",
    "<br/>\n",
    "<img src=\"assets/qa_docs.png\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step. 1 - Load the document(s)\n",
    "Specify a `DocumentLoader` to load in your unstructured data as `Documents`. A `Document` is a piece of text (the page_content) and associated metadata."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "# NOTICE: this loader is not specifically designed for Wikipedia, it is just an example\n",
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/2022_FIFA_World_Cup\")\n",
    "data = loader.load()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step. 2 - Split\n",
    "Split the `Document` into chunks for embedding and vector storage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "print(len(all_splits))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step. 3 - Store\n",
    "To be able to look up our document splits, we first need to store them where we can later look them up. The most common way to do this is to embed the contents of each document then store the embedding and document in a vector store, with the embedding being used to index the document.\n",
    "\n",
    "NOTICE: Azure OpenAI embedding models currently only support batches of, at most, 16 chunks. For such reason, we set the `chunk_size` to 16."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embedding = OpenAIEmbeddings(\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_api_base=openai_api_base,\n",
    "    openai_api_type=openai_api_type,\n",
    "    openai_api_version=openai_api_version,\n",
    "    deployment=embedding_deployment_name,\n",
    "    chunk_size=16,\n",
    ")\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=embedding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "16"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.chunk_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4. - Retrieve\n",
    "Retrieve relevant splits for any question using similarity search. By default, langchain retrieves the top 4 docs. Later on we will increase this number to increase the accuracy of the answer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "data": {
      "text/plain": "4"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Who won 2022 world cup\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 5. Generate\n",
    "Distill the retrieved documents into an answer using an LLM/Chat model with `RetrievalQA` chain.\n",
    "\n",
    "In this example we customized our prompt, for didactic purpose. This is however not mandatory.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_api_base=openai_api_base,\n",
    "    openai_api_type=openai_api_type,\n",
    "    openai_api_version=openai_api_version,\n",
    "    deployment_name=model_deployment_name,\n",
    "    temperature=0\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "{'query': 'Who won 2022 world cup',\n 'result': 'Argentina won the 2022 World Cup.'}"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Don't try to make up the answer, only use the context to answer the question.\n",
    "The pieces of context refer to the Football World Cup 2022.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    # by default, langchain retrieves the top 4 chunks, here we prefer to\n",
    "    # retrieve more chunks to increase the chances of finding the answer\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 12}),\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    ")\n",
    "\n",
    "qa_chain({\"query\": question})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "{'query': 'Summarize the final of the 2022 world cup with a few sentences with a lot of details',\n 'result': 'In the final of the 2022 World Cup, Argentina and France played to a thrilling 3-3 draw after extra time, with Argentina ultimately winning 4-2 on penalties. The match was held at an unspecified venue and was attended by a total of 3,404,252 fans throughout the tournament. Kylian Mbappé was the top scorer of the tournament with 8 goals, while Lionel Messi was named the best player and Enzo Fernández was named the best young player. Emiliano was named the best goalkeeper.'}"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({\"query\": \"Summarize the final of the 2022 world cup with a few sentences with a lot of details\"})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "{'query': 'Tell me about the open ceremony of the world cup 2022',\n 'result': \"The opening ceremony of the 2022 FIFA World Cup took place on November 20, 2022 at the Al Bayt Stadium in Al Khor. It featured appearances by Morgan Freeman and Ghanim Al-Muftah, as well as performances by South Korean singer Jungkook and Qatari singer Fahad Al Kubaisi. The recitation of the Qur'an was also included for the first time in the opening ceremony.\"}"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({\"query\": \"Tell me about the open ceremony of the world cup 2022\"})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "{'query': 'Which team did the the Netherlands play after the groups?',\n 'result': 'The Netherlands played the United States in the round of 16.'}"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({\"query\": \"Which team did the the Netherlands play after the groups?\"})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "data": {
      "text/plain": "{'query': 'How about Italy? Did they participate in the tournament?',\n 'result': 'No, Italy did not qualify for the 2022 Football World Cup.'}"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sad question ...\n",
    "qa_chain({\"query\": \"How about Italy? Did they participate in the tournament?\"})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "{'query': 'After France won the final, what happened?',\n 'result': 'France did not win the final, Argentina won the final against France on penalties.'}"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tricky question\n",
    "qa_chain({\"query\": \"After France won the final, what happened?\"})"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
