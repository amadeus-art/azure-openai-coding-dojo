{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/amadeus-art/azure-openai-coding-dojo/blob/main/azure_openai_qa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "# Question Answering on Documents using Azure OpenAI, Langchain and ChromaDB\n",
    "\n",
    "One of the main problems of Large Language Models (LLMs) is that they hallucinate (produce inaccurate or false information) when asked questions that are out of their scope. Also, their knowledge is up-to-date only if they are retrained or fine-tuned on recent data.\n",
    "\n",
    "In this tutorial we will showcase how to use the `langchain` library to perform question answering using \"knowledge base informed large language models\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install dependencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "!pip install openai langchain python-dotenv chromadb -q"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Environment setup\n",
    "Before executing the following cells, make sure to set the `AZURE_OPENAI_KEY` and `AZURE_OPENAI_ENDPOINT` variables in the `.env` file or export them.\n",
    "\n",
    "<br/>\n",
    "<img src=\"assets/keys_endpoint.png\" width=\"800\"/>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())  # read local .env file\n",
    "\n",
    "openai_api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "openai_api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  # should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/\n",
    "openai_api_type = 'azure'\n",
    "openai_api_version = '2023-07-01-preview'  # latest as per today (15-09-2023), may change in the future\n",
    "\n",
    "# these are the name of the deployments you created in the Azure portal within the above resource\n",
    "model_deployment_name = os.getenv(\"MODEL_DEPLOYMENT_NAME\")\n",
    "embedding_deployment_name = os.getenv(\"EMBEDDING_DEPLOYMENT_NAME\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Example of LLM response on a question after its knowledge cut"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI language model, I cannot predict the future. The 2022 football world cup has not yet taken place. It is scheduled to be held in Qatar from November 21 to December 18, 2022. \n",
      "\n",
      "As an AI language model, I don't have access to future events. The finalists of the 2022 football world cup are yet to be determined.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_api_base=openai_api_base,\n",
    "    openai_api_type=openai_api_type,\n",
    "    openai_api_version=openai_api_version,\n",
    "    deployment_name=model_deployment_name,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "question_1 = HumanMessage(content=\"Who won the 2022 football world cup?\")\n",
    "question_2 = HumanMessage(content=\"List the finalists of the 2022 football world cup\")\n",
    "\n",
    "print(llm([question_1]).content, \"\\n\")\n",
    "print(llm([question_2]).content)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step. 1 - Load the document(s)\n",
    "Specify a `DocumentLoader` to load in your unstructured data as `Documents`. A `Document` is a piece of text (the page_content) and associated metadata."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/2022_FIFA_World_Cup\")\n",
    "data = loader.load()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step. 2 - Split\n",
    "Split the `Document` into chunks for embedding and vector storage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "print(len(all_splits))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step. 3 - Store\n",
    "To be able to look up our document splits, we first need to store them where we can later look them up. The most common way to do this is to embed the contents of each document then store the embedding and document in a vector store, with the embedding being used to index the document.\n",
    "\n",
    "NOTICE: Azure OpenAI embedding models currently only support batches of, at most, 16 chunks. For such reason, we set the `chunk_size` to 16."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embedding = OpenAIEmbeddings(\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_api_base=openai_api_base,\n",
    "    openai_api_type=openai_api_type,\n",
    "    openai_api_version=openai_api_version,\n",
    "    deployment=embedding_deployment_name,\n",
    "    chunk_size=16,\n",
    ")\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=embedding)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "16"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.chunk_size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4. - Retrieve\n",
    "Retrieve relevant splits for any question using similarity search. By default, langchain retrieves the top 4 docs. Later on we will increase this number to increase the accuracy of the answer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "4"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Who won 2022 world cup\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5. Generate\n",
    "Distill the retrieved documents into an answer using an LLM/Chat model with `RetrievalQA` chain.\n",
    "\n",
    "In this example we customized our prompt, for didactic purpose. This is however not mandatory.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_key=openai_api_key,\n",
    "    openai_api_base=openai_api_base,\n",
    "    openai_api_type=openai_api_type,\n",
    "    openai_api_version=openai_api_version,\n",
    "    deployment_name=model_deployment_name,\n",
    "    temperature=0\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "{'query': 'Who won 2022 world cup',\n 'result': 'Argentina won the 2022 World Cup.'}"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Don't try to make up the answer, only use the context to answer the question.\n",
    "The pieces of context refer to the Football World Cup 2022.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    # by default, langchain retrieves the top 4 chunks, here we prefer to\n",
    "    # retrieve more chunks to increase the chances of finding the answer\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 12}),\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "\n",
    "qa_chain({\"query\": question})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "{'query': 'Summarize the final of the 2022 world cup with a few sentences with a lot of details',\n 'result': 'In the final of the 2022 World Cup, Argentina faced off against France in a thrilling match that ended in a 3-3 draw after extra time. The match was eventually decided by penalties, with Argentina emerging as the champions after a 4-2 victory. Lionel Messi was named the best player of the tournament, while Kylian Mbapp√© finished as the top scorer with 8 goals.'}"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({\"query\": \"Summarize the final of the 2022 world cup with a few sentences with a lot of details\"})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "{'query': 'Which teams did France face in the group stage?',\n 'result': 'France faced Australia, Tunisia, and Denmark in the group stage.'}"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain({\"query\": \"Which teams did France face in the group stage?\"})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "{'query': 'How about Italy? Did they participate in the tournament?',\n 'result': 'No, Italy did not participate in the tournament as they failed to qualify for the second successive World Cup.'}"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sad question ...\n",
    "qa_chain({\"query\": \"How about Italy? Did they participate in the tournament?\"})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "{'query': 'After France won the final, what happened?',\n 'result': 'France did not win the final, Argentina won the final 4-2 on penalties following a 3-3 draw after extra time.'}"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tricky question\n",
    "qa_chain({\"query\": \"After France won the final, what happened?\"})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
