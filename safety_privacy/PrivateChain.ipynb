{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety and Privacy with LangChain\n",
    "\n",
    "In this notebook we will use our privacy (PII entity recognizer and anonymizer) and toxic text classifier `tensor-trek/distilbert-toxicity-classifier` along with LangChain to implement checks of the text going into our LLM and text generated by the LLM. For this example we will use HuggingFace Hub LLM with LangChain and a custom `PrivacyAndSafetyChain` chain that implements the two checks.\n",
    "\n",
    "\n",
    "Let's install some dependencies first\n",
    "- You will need `transformers`, PyTorch, `langchain`, `presidio-analyzer`, `presidio-anonymizer`, and `spacy` libraries\n",
    "- You will also need the spacy `en_core_web_lg` model. You can also work with `en_core_web_md` model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install pip-system-certs -q\n",
    "!pip install openai langchain==0.1 transformers==4.28.0 -q\n",
    "!pip install presidio-analyzer presidio-anonymizer spacy huggingface-hub -q\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv())  # read local .env file\n",
    "\n",
    "openai_api_version = '2023-08-01-preview'\n",
    "model_deployment_name = os.getenv('MODEL_DEPLOYMENT_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the `PrivacyAndSafetyChain` custom chain\n",
    "\n",
    "The directory `PrivacyAndSafety` contains files that implements the custom Chain.\n",
    "- The `privacy_and_safety.py` file contains a Subclass of the base LangChain `Chain` class\n",
    "- The `check.py` file contains the actual toxic text classification and PII entity detection and anonymization\n",
    "\n",
    "Let's import and initialize `PrivacyAndSafetyChain` first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Configuration for `PrivacyAndSafetyChain`\n",
    "\n",
    "We can customize the behavior of `PrivacyAndSafetyChain` via the following parameters\n",
    "\n",
    "- `pii_mask_character`, the character used to perform anonymization of PII entities. Default is `*`\n",
    "- `pii_labels` if you wish to specify a specific list of PII entity types, then a list of entity types. For a full list of PII entity labels refer [Presidio supported entities](https://microsoft.github.io/presidio/supported_entities/). Defaults to ALL entities.\n",
    "- `fail_on_pii` a boolean flag which will make the chain fail if PII is detected. Defaults to `False`.\n",
    "- `pii_threshold` the confidence score threshold for PII entity recognition. Defaults to 50%\n",
    "- `toxicity_threshold` the confidence score threshold for toxicity classification. Defaults to 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chains import PrivacyAndSafetyChain\n",
    "\n",
    "safety_privacy = PrivacyAndSafetyChain(\n",
    "    verbose=True,\n",
    "    pii_mask_character=\"#\",\n",
    "    pii_labels = [\"PHONE_NUMBER\", \"EMAIL_ADDRESS\", \"PERSON\", \"US_SSN\"],\n",
    "    fail_on_pii = True,\n",
    "    pii_threshold = 0.5,\n",
    "    toxicity_threshold = 0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "# gets the API Key from environment variable AZURE_OPENAI_API_KEY\n",
    "client = AzureOpenAI(\n",
    "    api_version=openai_api_version,\n",
    ")\n",
    "\n",
    "\n",
    "# this is the name of the deployments you created in the Azure portal within the above resource\n",
    "from typing import List, Dict\n",
    "\n",
    "def get_chat_with_conversation(\n",
    "        text,\n",
    "        temperature: float = 0.2,\n",
    "        **model_kwargs\n",
    ") -> str:\n",
    "    try:\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": '\"\"\"'+ str(text) + '\"\"\"'}\n",
    "        ]\n",
    "        response = client.chat.completions.create(model=model_deployment_name,\n",
    "                                                  messages=messages)\n",
    " \n",
    "        return response.choices[0].message.content\n",
    "    except openai.OpenAIError as e: # this is the base class of any openai exception\n",
    "        print(f\"The call to the Chat Completion API failed as a consequence \"\n",
    "              f\"of the following exception: {e}\")\n",
    "\n",
    "        \n",
    "def user_request():\n",
    "    # Take request\n",
    "    request = input(\"\\nEnter an instruction\"\n",
    "                    \"(or 'quit'):\")\n",
    "    if request.lower() == \"quit\":\n",
    "        raise KeyboardInterrupt()\n",
    "    return request\n",
    "\n",
    "def user_reply_success(request,response):\n",
    "    # Create and print user reply\n",
    "    reply = f\"{request}:\\n{response}\"\n",
    "    print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangCahin Expression Language (LCEL)\n",
    "\n",
    "<div>\n",
    "<img src=\"../assets/LCEL1.png\" width=\"500\" align=\"left\"/>&nbsp;&nbsp;<img src=\"../assets/LCEL2.png\" width=\"540\" align=\"right\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's wrap it up together and create our app using LangChain Expression Language (LCEL).\n",
    "\n",
    "The chain is now containing :\n",
    "\n",
    "<br>\n",
    "<img src=\"../assets/PrivacySafetyChain.png\" width=\"950\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new PrivacyAndSafetyChain chain...\u001B[0m\n",
      "Running PrivacyAndSafetyChain...\n"
     ]
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/320 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aec5acd58577488e8210fe6a6d55eae8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "58b03346777a411ca010cf07d3fd34d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "67e353e125334bc9aad91033b9f8bfd1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd877e40a26144aa99b5c02afd5565b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5de78d363ea7423f9da818aaba4434c1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8385de813a184c09b8532da615eb5911"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for Toxic content...\n",
      "Toxic content found in text. Stopping...\n",
      "Toxic content found in text. Stopping...\n",
      "\n",
      "\n",
      "\n",
      "Quit!\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"{question}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "chain = (\n",
    "    prompt \n",
    "    | safety_privacy \n",
    "    | {\"input\": (lambda x: x['output'] )}\n",
    "    | get_chat_with_conversation\n",
    "    | safety_privacy \n",
    ")\n",
    "\n",
    "\n",
    "while True : \n",
    "    try:\n",
    "        input_user = user_request()\n",
    "        response = chain.invoke({\"question\": input_user})\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nQuit!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "    else:\n",
    "\n",
    "        print(\"\\n The answer is: \\n\", response['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of prompts:\n",
    "\n",
    "##### I hate my coworkers, can you help me get them fired?\n",
    "\n",
    "\n",
    "\"\"\"What is John Doe's email address, phone number and SSN from the following text?\n",
    "John Doe, a resident of 1234 Elm Street in Springfield, recently celebrated his birthday on January 1st. Turning 43 this year, John reflected on the years gone by. He often shares memories of his younger days with his close friends through calls on his phone, (555) 123-4567. Meanwhile, during a casual evening, he received an email at johndoe@example.com reminding him of an old acquaintance's reunion. As he navigated through some old documents, he stumbled upon a paper that listed his SSN as 338-12-6789, reminding him to store it in a safer place.\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chains import PrivacyAndSafetyChain\n",
    "\n",
    "safety_privacy = PrivacyAndSafetyChain(\n",
    "    verbose=True,\n",
    "    pii_mask_character=\"#\",\n",
    "    pii_labels = [\"PHONE_NUMBER\", \"EMAIL_ADDRESS\", \"PERSON\", \"US_SSN\"],\n",
    "    fail_on_pii = False,\n",
    "    pii_threshold = 0.5,\n",
    "    toxicity_threshold = 0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"{question}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "chain = (\n",
    "    prompt \n",
    "    | safety_privacy \n",
    "    | {\"input\": (lambda x: x['output'] )}\n",
    "    | get_chat_with_conversation\n",
    "    | safety_privacy \n",
    ")\n",
    "\n",
    "\n",
    "while True : \n",
    "    try:\n",
    "        input_user = user_request()\n",
    "        response = chain.invoke({\"question\": input_user})\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nQuit!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "    else:\n",
    "        print(\"\\n The answer is: \\n\", response['output'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
